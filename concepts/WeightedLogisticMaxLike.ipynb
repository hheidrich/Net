{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from scipy.io import mmread\n",
    "from scipy.stats import entropy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.eye(3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "p_stationary = np.real(eigs(A.T, k=1, sigma=1.0000001)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stationary.T @ A - p_stationary.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "Sample $(x_i, y_i)\\sim W$ from a joint distribution $W\\in\\mathbb{R}^{N\\times N}$. For a matrix $L\\in\\mathbb{R}^{N\\times N}$, we take the softmax $\\sigma$ _row-wise_ to make it a conditional distribution, and look at the maximum likelihood objective\n",
    "\n",
    "$$\n",
    "\\text{max}_L \\prod_{x_i, y_i}\\sigma(L)_{y_i | x_i}\\,.\n",
    "$$\n",
    "\n",
    "Taking $-\\text{log}$, the objective reformulates to the loss\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\text{min}_L -\\sum_{x_i, y_i}\\log\\left(\\sigma(L)_{y_i | x_i}\\right)\\\\\n",
    "\\Leftrightarrow\\quad &\\text{min}_L  -\\sum_{i, j}\\frac{\\#\\{(x, y) = (i, j)\\}}{\\# \\text{samples}}\\log\\sigma(L)_{j | i}\\\\\n",
    "\\Leftrightarrow\\quad &\\text{min}_L  -\\sum_{i, j}w_{i,j}\\log\\sigma(L)_{j | i}\\\\\n",
    "\\Leftrightarrow\\quad &\\text{min}_L  -\\mathbb{E}_{W(x, y)}\\left[\\log\\sigma(L)(y | x)\\right]\\\\\n",
    "\\Leftrightarrow\\quad &\\text{min}_L -\\mathbb{E}_{W(x, y)}\\left[\\log W(y | x)\\right]\n",
    "+ \\mathbb{E}_{W(x, y)}\\left[\\log\\frac{W(y|x)}{\\sigma(L)(y | x)}\\right]\\\\\n",
    "\\Leftrightarrow\\quad &\\text{min}_L\\quad H_W(Y | X)\n",
    "\\quad +\\quad D\\left(W(Y|X) \\;||\\; \\sigma(L)(Y|X)\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "So we need to minimize the loss that consists of conditional entropy $H_W(Y | X)$ for the target distribution, and conditional relative entropy $D\\left(W(Y|X) \\;||\\; \\sigma(L)(Y|X)\\right)$ between target distribution and learned conditional distribution $\\sigma(L)$.\n",
    "\n",
    "For the gradient $\\nabla F$ of the loss $F$, we need to split the joint distribution $W$ into marginal $M$ (diagonal matrix) and conditional distribution $P$, s.t. $W = M \\cdot P$. The gradient is then (just plug in softmax and compute $\\partial L_{i^\\prime, j^\\prime}$) given by\n",
    "\n",
    "$$\n",
    "\\nabla F(L) = M \\cdot \\left(\\sigma(L) - P\\right)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_logreg_grad(L, P, M):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the weighted cross-entropy loss in L with weight matrix M * P.\n",
    "    Parameters\n",
    "    ----------\n",
    "    L: np.array of shape (N, N)\n",
    "            Logits of learnable (low rank) transition matrix.\n",
    "    M: diagonal np.array of shape (N, N)\n",
    "            Marginal of weight distribution.\n",
    "    P: np.array of shape (N, N)\n",
    "            Transition matrix of weight distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad: np.array of shape (N, N)\n",
    "            Gradient of loss at L.\n",
    "    \"\"\"\n",
    "    grad = M @ (softmax(L, axis=-1) - P)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_logreg_loss(L, P, M):\n",
    "#     \"\"\"\n",
    "#     Computes the weighted cross-entropy loss in L with weight matrix M * P.\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     L: np.array of shape (N, N)\n",
    "#             Logits of learnable (low rank) transition matrix.\n",
    "#     M: diagonal np.array of shape (N, N)\n",
    "#             Marginal of weight distribution.\n",
    "#     P: np.array of shape (N, N)\n",
    "#             Transition matrix of weight distribution.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     loss: float\n",
    "#             Loss at L.\n",
    "#     \"\"\"    \n",
    "#     W = M @ P\n",
    "#     M_eL = np.diag(np.log(np.sum(np.exp(L), axis=-1)))\n",
    "#     loss = np.sum(M_eL @ W - L * W)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_logreg_loss(L, W):\n",
    "    \"\"\"\n",
    "    Computes the weighted cross-entropy loss in L with weight matrix M * P.\n",
    "    Parameters\n",
    "    ----------\n",
    "    L: np.array of shape (N, N)\n",
    "            Logits of learnable (low rank) transition matrix.\n",
    "    W: np.array of shape (N, N)\n",
    "            Matrix of weight distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "            Loss at L.\n",
    "    \"\"\"    \n",
    "    d = np.log(np.exp(L).sum(axis=-1, keepdims=True))\n",
    "    loss = np.sum(W * (d * np.ones_like(W) - L))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    def __init__(self, P, M):\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.L = np.zeros_like(P)\n",
    "        self.step = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.L, softmax(self.L, axis=-1)\n",
    "        \n",
    "    def train(self, steps, lr):\n",
    "        for self.step in range(self.step, self.step + steps):\n",
    "            self.L -= lr * weighted_logreg_grad(self.L, self.P, self.M)\n",
    "            loss = weighted_logreg_loss(self.L, self.P, self.M)\n",
    "#             if self.step % 50 == 50-1:\n",
    "#                 print(f'Step: {self.step}, Loss: {loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_relative_entropy(W):\n",
    "    D = np.diag(1 / np.sum(W, axis=-1))\n",
    "    P = D @ W\n",
    "    entropy = -np.sum(W[W!=0] * np.log(P[W!=0]))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Learning the adjacency matrix for Zachary's Karate Club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = mmread('../data/soc-karate.mtx')\n",
    "A = G.toarray()\n",
    "\n",
    "W = A / A.sum()\n",
    "# d = np.sum(A, axis=-1)\n",
    "# M = np.diag(d) / d.sum()\n",
    "# P = (1 / np.expand_dims(d, -1)) * A\n",
    "\n",
    "# W = M @ P # = A / A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(P=P, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weighted_logreg_loss() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ee9168655abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-f7472833ae1c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps, lr)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_logreg_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_logreg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#             if self.step % 50 == 50-1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#                 print(f'Step: {self.step}, Loss: {loss:.5f}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: weighted_logreg_loss() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "logreg.train(steps=500, lr=1)\n",
    "end_time = time.time()\n",
    "\n",
    "L, P_L = logreg()\n",
    "print(f\"Loss: {weighted_logreg_loss(L=L, P=P, M=M)} in {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference to best loss is 0.0779378362012868\n"
     ]
    }
   ],
   "source": [
    "best_loss = conditional_relative_entropy(W)\n",
    "print('Difference to best loss is {}'.format(weighted_logreg_loss(L=L, P=P, M=M)-best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.788998746671057"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_relative_entropy(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.866936582872344"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_logreg_loss(L=L, W=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-100.           -5.04985601   -5.04985601 ...   -5.04985601\n",
      "  -100.         -100.        ]\n",
      " [  -5.04985601 -100.           -5.04985601 ... -100.\n",
      "  -100.         -100.        ]\n",
      " [  -5.04985601   -5.04985601 -100.         ... -100.\n",
      "    -5.04985601 -100.        ]\n",
      " ...\n",
      " [  -5.04985601 -100.         -100.         ... -100.\n",
      "    -5.04985601   -5.04985601]\n",
      " [-100.         -100.           -5.04985601 ...   -5.04985601\n",
      "  -100.           -5.04985601]\n",
      " [-100.         -100.         -100.         ...   -5.04985601\n",
      "    -5.04985601 -100.        ]]\n"
     ]
    }
   ],
   "source": [
    "L_W = -100 * np.ones_like(W)\n",
    "L_W[W!=0] = np.log(W[W!=0])\n",
    "print(L_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.788998746671057"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_logreg_loss(L=L_W, W=W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2) Joint distributions without zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017578789977912472\n"
     ]
    }
   ],
   "source": [
    "N = 34\n",
    "alpha = 3\n",
    "\n",
    "P_trans = np.random.dirichlet(alpha * np.ones(N), size=N)\n",
    "M=np.eye(N)/N\n",
    "print(np.min(P_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(P=P_trans, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.train(steps=5000, lr=0.1)\n",
    "\n",
    "L, P_L = logreg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3687511020399263\n"
     ]
    }
   ],
   "source": [
    "best_loss = conditional_relative_entropy(M @ P_trans)\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4336667704710555"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_logreg_loss(L=L, P=P_trans, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference to best loss is 0.06491566843112917\n"
     ]
    }
   ],
   "source": [
    "print('Difference to best loss is {}'.format(weighted_logreg_loss(L=L, P=P_trans, M=M)-best_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

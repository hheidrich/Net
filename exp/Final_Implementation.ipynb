{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "import os\n",
    "import abc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import save_npz, load_npz, csr_matrix\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# from netgan.netgan import *\n",
    "# from netgan import utils\n",
    "\n",
    "from net.utils import *\n",
    "from net import utils_netgan as utils\n",
    "import net.net as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "_A_obs, _X_obs, _z_obs = utils.load_npz('../data/cora_ml.npz')\n",
    "#_A_obs = load_npz('../data/gemsec.npz')\n",
    "_A_obs = _A_obs + _A_obs.T\n",
    "_A_obs[_A_obs > 1] = 1\n",
    "lcc = utils.largest_connected_components(_A_obs)\n",
    "_A_obs = _A_obs[lcc,:][:,lcc]\n",
    "_N = _A_obs.shape[0]\n",
    "\n",
    "val_share = 0.1\n",
    "test_share = 0.05\n",
    "seed = 481516234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ones, val_ones, val_zeros, test_ones, test_zeros = utils.train_val_test_split_adjacency(_A_obs, val_share, test_share, seed, undirected=True, connected=True, asserts=False)\n",
    "\n",
    "train_graph = sp.coo_matrix((np.ones(len(train_ones)),(train_ones[:,0], train_ones[:,1]))).tocsr()\n",
    "assert (train_graph.toarray() == train_graph.toarray().T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(abc.ABC):\n",
    "    def __init__(self, invoke_every):\n",
    "        self.training_stopped = False\n",
    "        self.invoke_every = invoke_every\n",
    "        print()\n",
    "        \n",
    "    def __call__(self, loss, model):\n",
    "        if model.step % self.invoke_every == 0:\n",
    "            self.invoke(loss, model)\n",
    "        \n",
    "    def stop_training(self):\n",
    "        self.training_stopped = True\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def invoke(self, loss, model):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OverlapTracker(Callback):\n",
    "    \"\"\"\n",
    "    This callback serves in three ways:\n",
    "    - It samples a graph from the model and saves it on hard drive.\n",
    "    - It tracks the EdgeOverlap and stops if the limit is met.\n",
    "    \"\"\"\n",
    "    def __init__(self, logdir=None, invoke_every=100, EO_limit=1.):\n",
    "        super().__init__(invoke_every)\n",
    "        self.logdir = logdir\n",
    "        if self.logdir is None:\n",
    "            self.logs = []\n",
    "        self.EO_limit = EO_limit\n",
    "\n",
    "    def invoke(self, loss, model):\n",
    "        sampled_graph = model.sample_graph()\n",
    "        # TODO: tune edge_overlap func\n",
    "        overlap = utils.edge_overlap(model.A.numpy(), sampled_graph) / model.num_edges\n",
    "        \n",
    "        \n",
    "        step_str = f'{model.step:{model.step_str_len}d}'\n",
    "        print(f'Step: {step_str}/{model.steps}, Loss: {loss:.5f}, Edge-Overlap: {overlap:.3f}')\n",
    "        if overlap >= self.EO_limit:\n",
    "            self.stop_training()\n",
    "            \n",
    "        if self.logdir:\n",
    "            filename = f'graph_{model.step:0{model.step_str_len}d}'\n",
    "            save_npz(file=os.path.join(self.logdir, filename),\n",
    "                     matrix=sampled_graph)\n",
    "        else:\n",
    "            self.logs.append(sampled_graph)\n",
    "\n",
    "\n",
    "class WeightWatcher(Callback):\n",
    "    \"\"\"\n",
    "    Saves the model's weights on hard drive.\n",
    "    \"\"\"\n",
    "    def __init__(self, logdir, invoke_every=100):\n",
    "        super().__init__(invoke_every)\n",
    "        self.logdir = logdir\n",
    "        \n",
    "    def invoke(self, loss, model):\n",
    "        filename =  f'weights_{model.step:0{model.step_str_len}d}'\n",
    "        np.savez(file=os.path.join(self.logdir, filename),\n",
    "                 W_down=model.W_down.detach().numpy(),\n",
    "                 W_up=model.W_up.detach().numpy())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self, A, H, callbacks=[]):\n",
    "        self.num_edges = A.sum()\n",
    "        self.A = torch.tensor(A)\n",
    "        self.step = 1\n",
    "        self.callbacks = callbacks\n",
    "        self._optimizer = None\n",
    "        N = A.shape[0]\n",
    "        gamma = np.sqrt(2/(N+H))\n",
    "        self.W_down = (gamma * torch.randn(N, H, device=device, dtype=dtype)).clone().detach().requires_grad_()\n",
    "        self.W_up = (gamma * torch.randn(H, N, device=device, dtype=dtype)).clone().detach().requires_grad_()\n",
    "        self.total_time = 0\n",
    "              \n",
    "    def __call__(self):\n",
    "        return torch.nn.functional.softmax(self.get_W(), dim=-1).detach().numpy()\n",
    "    \n",
    "    def get_W(self):\n",
    "        W = torch.mm(self.W_down, self.W_up)\n",
    "        W -= W.max(dim=-1, keepdims=True)[0]\n",
    "        #if self.force_W_symmetric:\n",
    "        #    W = torch.max(W, W.T)\n",
    "        return W\n",
    "    \n",
    "    def loss(self, W):\n",
    "        \"\"\"\n",
    "        Computes the weighted cross-entropy loss in logits with weight matrix M * P.\n",
    "        Parameters\n",
    "        ----------\n",
    "        W: torch.tensor of shape (N, N)\n",
    "                Logits of learnable (low rank) transition matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: torch.tensor (float)\n",
    "                Loss at logits.\n",
    "        \"\"\"\n",
    "        d = torch.log(torch.exp(W).sum(dim=-1, keepdims=True))\n",
    "        loss = torch.sum(self.A * (d * torch.ones_like(self.A) - W)) / self.num_edges\n",
    "        return loss \n",
    "    \n",
    "    def _closure(self):\n",
    "        W = self.get_W()\n",
    "        loss = self.loss(W=W)\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        \n",
    "    def _train_step(self):\n",
    "        time_start = time.time()\n",
    "        loss = self._optimizer.step(self._closure)\n",
    "        time_end = time.time()\n",
    "        return loss.item(), (time_end - time_start)\n",
    "    \n",
    "    def train(self, steps, optimizer_fn, optimizer_args, EO_criterion=None):\n",
    "        self._optimizer = optimizer_fn([self.W_down, self.W_up], **optimizer_args)\n",
    "        self.steps = steps\n",
    "        self.step_str_len = int(np.log10(steps))+1\n",
    "        stop = False\n",
    "        for self.step in range(self.step, steps+self.step):\n",
    "            loss, time = self._train_step()\n",
    "            self.total_time += time\n",
    "            for callback in self.callbacks:\n",
    "                callback(loss=loss, model=self)\n",
    "                stop = stop or callback.training_stopped    \n",
    "            if stop: break\n",
    "                \n",
    "    def sample_graph(self):\n",
    "        transition_matrix = self()\n",
    "        scores_matrix = scores_matrix_from_transition_matrix(transition_matrix=transition_matrix,\n",
    "                                                             symmetric=True)\n",
    "        sampled_graph = utils.graph_from_scores(scores_matrix, self.num_edges)\n",
    "        return sampled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netmodel = Net(A=train_graph.toarray(),\n",
    "               H=12,\n",
    "               callbacks=[OverlapTracker(logdir='../logs/sampled_graphs',\n",
    "                                         invoke_every=5,\n",
    "                                         EO_limit=.5),\n",
    "                          WeightWatcher(logdir='../logs/weights',\n",
    "                                        invoke_every=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   5/400, Loss: 7.39126, Edge-Overlap: 0.006\n",
      "Step:  10/400, Loss: 5.35504, Edge-Overlap: 0.092\n",
      "Step:  15/400, Loss: 4.02432, Edge-Overlap: 0.355\n",
      "Step:  20/400, Loss: 3.35688, Edge-Overlap: 0.456\n",
      "Step:  25/400, Loss: 2.97855, Edge-Overlap: 0.571\n",
      "14.361342430114746\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "netmodel.train(steps=400,\n",
    "               optimizer_fn=torch.optim.Adam,\n",
    "               optimizer_args={'lr': 0.1,\n",
    "                               'weight_decay': 1e-7})\n",
    "total = time.time() - start\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3384577906085626"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(total - netmodel.total_time) / (6.3-netmodel.total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.943687915802002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netmodel.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_npz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.csc_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function savez in module numpy:\n",
      "\n",
      "savez(file, *args, **kwds)\n",
      "    Save several arrays into a single file in uncompressed ``.npz`` format.\n",
      "    \n",
      "    If arguments are passed in with no keywords, the corresponding variable\n",
      "    names, in the ``.npz`` file, are 'arr_0', 'arr_1', etc. If keyword\n",
      "    arguments are given, the corresponding variable names, in the ``.npz``\n",
      "    file will match the keyword names.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    file : str or file\n",
      "        Either the file name (string) or an open file (file-like object)\n",
      "        where the data will be saved. If file is a string or a Path, the\n",
      "        ``.npz`` extension will be appended to the file name if it is not\n",
      "        already there.\n",
      "    args : Arguments, optional\n",
      "        Arrays to save to the file. Since it is not possible for Python to\n",
      "        know the names of the arrays outside `savez`, the arrays will be saved\n",
      "        with names \"arr_0\", \"arr_1\", and so on. These arguments can be any\n",
      "        expression.\n",
      "    kwds : Keyword arguments, optional\n",
      "        Arrays to save to the file. Arrays will be saved in the file with the\n",
      "        keyword names.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    save : Save a single array to a binary file in NumPy format.\n",
      "    savetxt : Save an array to a file as plain text.\n",
      "    savez_compressed : Save several arrays into a compressed ``.npz`` archive\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The ``.npz`` file format is a zipped archive of files named after the\n",
      "    variables they contain.  The archive is not compressed and each file\n",
      "    in the archive contains one variable in ``.npy`` format. For a\n",
      "    description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.\n",
      "    \n",
      "    When opening the saved ``.npz`` file with `load` a `NpzFile` object is\n",
      "    returned. This is a dictionary-like object which can be queried for\n",
      "    its list of arrays (with the ``.files`` attribute), and for the arrays\n",
      "    themselves.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from tempfile import TemporaryFile\n",
      "    >>> outfile = TemporaryFile()\n",
      "    >>> x = np.arange(10)\n",
      "    >>> y = np.sin(x)\n",
      "    \n",
      "    Using `savez` with \\*args, the arrays are saved with default names.\n",
      "    \n",
      "    >>> np.savez(outfile, x, y)\n",
      "    >>> _ = outfile.seek(0) # Only needed here to simulate closing & reopening file\n",
      "    >>> npzfile = np.load(outfile)\n",
      "    >>> npzfile.files\n",
      "    ['arr_0', 'arr_1']\n",
      "    >>> npzfile['arr_0']\n",
      "    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "    \n",
      "    Using `savez` with \\**kwds, the arrays are saved with the keyword names.\n",
      "    \n",
      "    >>> outfile = TemporaryFile()\n",
      "    >>> np.savez(outfile, x=x, y=y)\n",
      "    >>> _ = outfile.seek(0)\n",
      "    >>> npzfile = np.load(outfile)\n",
      "    >>> sorted(npzfile.files)\n",
      "    ['x', 'y']\n",
      "    >>> npzfile['x']\n",
      "    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.savez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:netgan]",
   "language": "python",
   "name": "conda-env-netgan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

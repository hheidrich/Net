{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import abc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import save_npz, load_npz, csr_matrix\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# from netgan.netgan import *\n",
    "# from netgan import utils\n",
    "\n",
    "from net.utils import *\n",
    "from net import utils_netgan as utils\n",
    "import net.net as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "_A_obs, _X_obs, _z_obs = utils.load_npz('../data/cora_ml.npz')\n",
    "#_A_obs = load_npz('../data/gemsec.npz')\n",
    "_A_obs = _A_obs + _A_obs.T\n",
    "_A_obs[_A_obs > 1] = 1\n",
    "lcc = utils.largest_connected_components(_A_obs)\n",
    "_A_obs = _A_obs[lcc,:][:,lcc]\n",
    "_N = _A_obs.shape[0]\n",
    "\n",
    "val_share = 0.1\n",
    "test_share = 0.05\n",
    "seed = 481516234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ones, val_ones, val_zeros, test_ones, test_zeros = utils.train_val_test_split_adjacency(_A_obs, val_share, test_share, seed, undirected=True, connected=True, asserts=False)\n",
    "\n",
    "train_graph = sp.coo_matrix((np.ones(len(train_ones)),(train_ones[:,0], train_ones[:,1]))).tocsr()\n",
    "assert (train_graph.toarray() == train_graph.toarray().T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(abc.ABC):\n",
    "    def __init__(self, invoke_every):\n",
    "        self.training_stopped = False\n",
    "        self.invoke_every = invoke_every\n",
    "        print()\n",
    "        \n",
    "    def __call__(self, loss, model):\n",
    "        if model.step % self.invoke_every == 0:\n",
    "            self.invoke(loss, model)\n",
    "        \n",
    "    def stop_training(self):\n",
    "        self.training_stopped = True\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def invoke(self, loss, model):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OverlapTracker(Callback):\n",
    "    \"\"\"\n",
    "    This callback serves in three ways:\n",
    "    - It samples a graph from the model and saves it on hard drive.\n",
    "    - It tracks the EdgeOverlap and stops if the limit is met.\n",
    "    - It tracks the total time\n",
    "    \"\"\"\n",
    "    def __init__(self, logdir=None, invoke_every=100, EO_limit=1.):\n",
    "        super().__init__(invoke_every)\n",
    "        self.logdir = logdir\n",
    "        if self.logdir is None:\n",
    "            self.logs = []\n",
    "        self.EO_limit = EO_limit\n",
    "        self.overlap_dict = {}\n",
    "        self.time_dict = {}\n",
    "        \n",
    "\n",
    "    def invoke(self, loss, model):\n",
    "        start = time.time()\n",
    "        sampled_graph = model.sample_graph()\n",
    "        # TODO: tune edge_overlap func\n",
    "        overlap = utils.edge_overlap(model.A.numpy(), sampled_graph) / model.num_edges\n",
    "        self.overlap_dict[model.step] = overlap\n",
    "        overlap_time = time.time() - start\n",
    "        model.total_time += overlap_time\n",
    "        self.time_dict[model.step] = model.total_time\n",
    "        \n",
    "        step_str = f'{model.step:{model.step_str_len}d}'\n",
    "        print(f'Step: {step_str}/{model.steps}, Loss: {loss:.5f}, Edge-Overlap: {overlap:.3f}')\n",
    "        if overlap >= self.EO_limit:\n",
    "            self.stop_training()\n",
    "            \n",
    "        if self.training_stopped or model.step==model.steps:\n",
    "            with open(os.path.join(self.logdir, 'overlap.pickle'), 'wb') as handle:\n",
    "                pickle.dump(self.overlap_dict,\n",
    "                            handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(os.path.join(self.logdir, 'timing.pickle'), 'wb') as handle:\n",
    "                pickle.dump(self.time_dict,\n",
    "                            handle,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        if self.logdir:\n",
    "            filename = f'graph_{model.step:0{model.step_str_len}d}'\n",
    "            save_npz(file=os.path.join(self.logdir, filename),\n",
    "                     matrix=sampled_graph)\n",
    "        else:\n",
    "            self.logs.append(sampled_graph)\n",
    "\n",
    "\n",
    "class WeightWatcher(Callback):\n",
    "    \"\"\"\n",
    "    Saves the model's weights on hard drive.\n",
    "    \"\"\"\n",
    "    def __init__(self, logdir, invoke_every=100):\n",
    "        super().__init__(invoke_every)\n",
    "        self.logdir = logdir\n",
    "        \n",
    "    def invoke(self, loss, model):\n",
    "        filename =  f'weights_{model.step:0{model.step_str_len}d}'\n",
    "        np.savez(file=os.path.join(self.logdir, filename),\n",
    "                 W_down=model.W_down.detach().numpy(),\n",
    "                 W_up=model.W_up.detach().numpy())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self, A, H, callbacks=[]):\n",
    "        self.num_edges = A.sum()\n",
    "        self.A = torch.tensor(A)\n",
    "        self.step = 1\n",
    "        self.callbacks = callbacks\n",
    "        self._optimizer = None\n",
    "        N = A.shape[0]\n",
    "        gamma = np.sqrt(2/(N+H))\n",
    "        self.W_down = (gamma * torch.randn(N, H, device=device, dtype=dtype)).clone().detach().requires_grad_()\n",
    "        self.W_up = (gamma * torch.randn(H, N, device=device, dtype=dtype)).clone().detach().requires_grad_()\n",
    "        self.total_time = 0\n",
    "              \n",
    "    def __call__(self):\n",
    "        return torch.nn.functional.softmax(self.get_W(), dim=-1).detach().numpy()\n",
    "    \n",
    "    def get_W(self):\n",
    "        W = torch.mm(self.W_down, self.W_up)\n",
    "        W -= W.max(dim=-1, keepdims=True)[0]\n",
    "        #if self.force_W_symmetric:\n",
    "        #    W = torch.max(W, W.T)\n",
    "        return W\n",
    "    \n",
    "    def loss(self, W):\n",
    "        \"\"\"\n",
    "        Computes the weighted cross-entropy loss in logits with weight matrix M * P.\n",
    "        Parameters\n",
    "        ----------\n",
    "        W: torch.tensor of shape (N, N)\n",
    "                Logits of learnable (low rank) transition matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: torch.tensor (float)\n",
    "                Loss at logits.\n",
    "        \"\"\"\n",
    "        d = torch.log(torch.exp(W).sum(dim=-1, keepdims=True))\n",
    "        loss = torch.sum(self.A * (d * torch.ones_like(self.A) - W)) / self.num_edges\n",
    "        return loss \n",
    "    \n",
    "    def _closure(self):\n",
    "        W = self.get_W()\n",
    "        loss = self.loss(W=W)\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        \n",
    "    def _train_step(self):\n",
    "        time_start = time.time()\n",
    "        loss = self._optimizer.step(self._closure)\n",
    "        time_end = time.time()\n",
    "        return loss.item(), (time_end - time_start)\n",
    "    \n",
    "    def train(self, steps, optimizer_fn, optimizer_args, EO_criterion=None):\n",
    "        self._optimizer = optimizer_fn([self.W_down, self.W_up], **optimizer_args)\n",
    "        self.steps = steps\n",
    "        self.step_str_len = int(np.log10(steps))+1\n",
    "        stop = False\n",
    "        for self.step in range(self.step, steps+self.step):\n",
    "            loss, time = self._train_step()\n",
    "            self.total_time += time\n",
    "            for callback in self.callbacks:\n",
    "                callback(loss=loss, model=self)\n",
    "                stop = stop or callback.training_stopped    \n",
    "            if stop: break\n",
    "                \n",
    "    def sample_graph(self):\n",
    "        transition_matrix = self()\n",
    "        scores_matrix = scores_matrix_from_transition_matrix(transition_matrix=transition_matrix,\n",
    "                                                             symmetric=True)\n",
    "        sampled_graph = utils.graph_from_scores(scores_matrix, self.num_edges)\n",
    "        return sampled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netmodel = Net(A=train_graph.toarray(),\n",
    "               H=12,\n",
    "               callbacks=[OverlapTracker(logdir='../logs/sampled_graphs',\n",
    "                                         invoke_every=5,\n",
    "                                         EO_limit=.5),\n",
    "                          WeightWatcher(logdir='../logs/weights',\n",
    "                                        invoke_every=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   5/400, Loss: 7.40182, Edge-Overlap: 0.004\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../logs/sampled_graphs/graph_005.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4c7f0ed247c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                \u001b[0moptimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                optimizer_args={'lr': 0.1,\n\u001b[0;32m----> 5\u001b[0;31m                                'weight_decay': 1e-7})\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-642f9d2bd00d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps, optimizer_fn, optimizer_args, EO_criterion)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a4905e70c6f9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, loss, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a4905e70c6f9>\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, loss, model)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'graph_{model.step:0{model.step_str_len}d}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             save_npz(file=os.path.join(self.logdir, filename),\n\u001b[0;32m---> 58\u001b[0;31m                      matrix=sampled_graph)\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/netgan/lib/python3.6/site-packages/scipy/sparse/_matrix_io.py\u001b[0m in \u001b[0;36msave_npz\u001b[0;34m(file, matrix, compressed)\u001b[0m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompressed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marrays_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marrays_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavez_compressed\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/netgan/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavez_compressed\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \"\"\"\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/netgan/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZIP_STORED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/netgan/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mzipfile_factory\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allowZip64'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/netgan/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../logs/sampled_graphs/graph_005.npz'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "netmodel.train(steps=400,\n",
    "               optimizer_fn=torch.optim.Adam,\n",
    "               optimizer_args={'lr': 0.1,\n",
    "                               'weight_decay': 1e-7})\n",
    "total = time.time() - start\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_experiments(num_experiments,\n",
    "                      experiment_root,\n",
    "                      train_graph,\n",
    "                      H,\n",
    "                      optimizer,\n",
    "                      optimizer_args,\n",
    "                      invoke_every,\n",
    "                      steps):\n",
    "    \"\"\"Start multiple experiments.\"\"\"\n",
    "    # create root folder\n",
    "    Path(experiment_root).mkdir(parents=True, exist_ok=True)\n",
    "    netmodels = []\n",
    "    for experiment in range(num_experiments):\n",
    "        # create experiment folder\n",
    "        path = os.path.join(experiment_root, f'Experiment_{experiment}')\n",
    "        \n",
    "        path_graphs = os.path.join(path, 'sampled_graphs')\n",
    "        Path(path_graphs).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        path_weights = os.path.join(path, 'weights')\n",
    "        Path(path_weights).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # initialize model\n",
    "        netmodel = Net(A=train_graph.toarray(),\n",
    "                       H=H,\n",
    "                       callbacks=[OverlapTracker(logdir=path_graphs,\n",
    "                                                 invoke_every=invoke_every,\n",
    "                                                 EO_limit=1.),\n",
    "                                  WeightWatcher(logdir=path_weights,\n",
    "                                                invoke_every=invoke_every)])\n",
    "        \n",
    "        # train model\n",
    "        print(f'Experiment_{experiment}')\n",
    "        netmodel.train(steps=steps,\n",
    "               optimizer_fn=optimizer,\n",
    "               optimizer_args=optimizer_args)\n",
    "        netmodels.append(netmodel)\n",
    "    return netmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experiment_0\n",
      "Step:   5/100, Loss: 7.40168, Edge-Overlap: 0.004\n",
      "Step:  10/100, Loss: 5.43492, Edge-Overlap: 0.098\n",
      "Step:  15/100, Loss: 4.12435, Edge-Overlap: 0.336\n",
      "Step:  20/100, Loss: 3.43191, Edge-Overlap: 0.438\n",
      "Step:  25/100, Loss: 3.03205, Edge-Overlap: 0.546\n",
      "Step:  30/100, Loss: 2.78196, Edge-Overlap: 0.617\n",
      "Step:  35/100, Loss: 2.61936, Edge-Overlap: 0.668\n",
      "Step:  40/100, Loss: 2.50718, Edge-Overlap: 0.711\n",
      "Step:  45/100, Loss: 2.42752, Edge-Overlap: 0.742\n",
      "Step:  50/100, Loss: 2.36856, Edge-Overlap: 0.764\n",
      "Step:  55/100, Loss: 2.32291, Edge-Overlap: 0.789\n",
      "Step:  60/100, Loss: 2.28682, Edge-Overlap: 0.804\n",
      "Step:  65/100, Loss: 2.25790, Edge-Overlap: 0.818\n",
      "Step:  70/100, Loss: 2.23396, Edge-Overlap: 0.826\n",
      "Step:  75/100, Loss: 2.21413, Edge-Overlap: 0.838\n",
      "Step:  80/100, Loss: 2.19739, Edge-Overlap: 0.854\n",
      "Step:  85/100, Loss: 2.18339, Edge-Overlap: 0.860\n",
      "Step:  90/100, Loss: 2.17119, Edge-Overlap: 0.869\n",
      "Step:  95/100, Loss: 2.16066, Edge-Overlap: 0.875\n",
      "Step: 100/100, Loss: 2.15145, Edge-Overlap: 0.878\n",
      "\n",
      "\n",
      "Experiment_1\n",
      "Step:   5/100, Loss: 7.40397, Edge-Overlap: 0.005\n",
      "Step:  10/100, Loss: 5.41114, Edge-Overlap: 0.087\n",
      "Step:  15/100, Loss: 4.05955, Edge-Overlap: 0.345\n",
      "Step:  20/100, Loss: 3.38128, Edge-Overlap: 0.461\n",
      "Step:  25/100, Loss: 2.99076, Edge-Overlap: 0.551\n",
      "Step:  30/100, Loss: 2.74087, Edge-Overlap: 0.627\n",
      "Step:  35/100, Loss: 2.58281, Edge-Overlap: 0.673\n",
      "Step:  40/100, Loss: 2.47432, Edge-Overlap: 0.721\n",
      "Step:  45/100, Loss: 2.39811, Edge-Overlap: 0.751\n",
      "Step:  50/100, Loss: 2.34227, Edge-Overlap: 0.770\n",
      "Step:  55/100, Loss: 2.30010, Edge-Overlap: 0.796\n",
      "Step:  60/100, Loss: 2.26744, Edge-Overlap: 0.812\n",
      "Step:  65/100, Loss: 2.24163, Edge-Overlap: 0.814\n",
      "Step:  70/100, Loss: 2.22057, Edge-Overlap: 0.837\n",
      "Step:  75/100, Loss: 2.20300, Edge-Overlap: 0.845\n",
      "Step:  80/100, Loss: 2.18807, Edge-Overlap: 0.853\n",
      "Step:  85/100, Loss: 2.17529, Edge-Overlap: 0.866\n",
      "Step:  90/100, Loss: 2.16416, Edge-Overlap: 0.874\n",
      "Step:  95/100, Loss: 2.15440, Edge-Overlap: 0.880\n",
      "Step: 100/100, Loss: 2.14595, Edge-Overlap: 0.885\n"
     ]
    }
   ],
   "source": [
    "models = start_experiments(num_experiments=2,\n",
    "                           experiment_root='../logs/experiments_CORA-ML',\n",
    "                           train_graph=train_graph,\n",
    "                           H=12,\n",
    "                           optimizer=torch.optim.Adam,\n",
    "                           optimizer_args={'lr': 0.1,\n",
    "                                           'weight_decay': 1e-7},\n",
    "                           invoke_every=5,\n",
    "                           steps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3384577906085626"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(total - netmodel.total_time) / (6.3-netmodel.total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.943687915802002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netmodel.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_npz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.csc_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:netgan]",
   "language": "python",
   "name": "conda-env-netgan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
